# Spark Protobuf Backport

The **spark‑protobuf‑backport** project brings the `from_protobuf` and `to_protobuf` functions introduced in Spark 3.4 to Spark 3.2.1.  It provides Catalyst expressions that convert between binary Protobuf payloads and Spark SQL values without requiring users to patch or rebuild Spark.  This report describes how the library infers the schema from a Protobuf definition, how it constructs and executes the `from_protobuf` and `to_protobuf` expressions (using the binary‑descriptor‑set approach to ensure executors see the descriptor), and how the actual parsing and serialisation work internally.

## Schema inference

When `from_protobuf` is called, the backport needs to understand the Protobuf message structure so that it can return a column with a rich Spark SQL type (usually a `StructType`).  Schema inference happens in three steps:

1. **Loading the message descriptor.**  The message definition may be provided via the fully qualified Java class name of a compiled Protobuf message or via a serialized descriptor file.  `ProtobufUtils.buildDescriptor` chooses the appropriate mechanism: when a descriptor file is supplied it reads the descriptor set and looks up the message by name; otherwise it loads the Java class and calls its `getDescriptor` method【790644388126332†L124-L133】.  It also verifies that the Java class extends the shaded Protobuf `Message` class and throws a descriptive error if shading is missing【790644388126332†L136-L166】.

2. **Converting to Spark SQL types.**  Once the descriptor is available, `SchemaConverters.toSqlType` walks the descriptor and constructs a Spark schema.  It wraps the result in a `SchemaType` whose `dataType` is a `StructType` with fields corresponding to each Protobuf field【122789228415541†L36-L47】.  The conversion handles primitive mappings (e.g., `int32` → `IntegerType`, `bool` → `BooleanType`), enums (represented as `StringType`), repeated fields (converted to `ArrayType`), map entries (converted to `MapType`), timestamps (converted to `TimestampType`), durations (converted to a `DayTimeIntervalType` because Spark 3.2 does not have a generic interval type) and nested messages (converted recursively to nested `StructType`s)【122789228415541†L56-L127】.  The `ProtobufOptions` object controls recursion: by default it uses unlimited recursion, but users can specify a maximum depth via `recursive.fields.max.depth` to avoid infinite cycles.  If the same record type appears beyond this depth the field is dropped, and an optional log message is emitted【122789228415541†L104-L117】.

3. **Attaching the inferred schema to the expression.**  In `ProtobufDataToCatalyst` the `dataType` of the expression is lazily computed as `SchemaConverters.toSqlType(messageDescriptor, protobufOptions).dataType`【222086645450284†L47-L54】.  This ensures that any parse‑mode or recursion options are taken into account when converting the descriptor.  The resulting Spark `StructType` (with nested arrays, maps and structs) becomes the type of the column returned by `from_protobuf`.

## Constructing `from_protobuf` and `to_protobuf` columns

### DataFrame API

The backport exposes DataFrame‑level functions in `org.apache.spark.sql.protobuf.backport.functions`.  The overloaded `from_protobuf` methods take a binary column, a message name or Java class, an optional descriptor file and an optional options map.  They return a new `Column` wrapping a `ProtobufDataToCatalyst` expression.  Similarly, `to_protobuf` takes a Catalyst column and message definition and wraps it in `CatalystDataToProtobuf`【630433556658279†L19-L37】【630433556658279†L84-L115】.  When the user provides options, they are passed into `ProtobufOptions` to set the parse mode (permissive or fail‑fast) and recursion limits.

For SQL users, `ProtobufExtensions` registers `from_protobuf` and `to_protobuf` as functions via `SparkSessionExtensions.injectFunction`.  The builder reads literal string arguments for the message/class name and descriptor file and constructs the appropriate expression【894934202369200†L31-L70】.  Additional arguments beyond the descriptor path are currently ignored in the SQL path; the DataFrame API supports options through the overloaded functions.

### Support for binary descriptor sets

Spark executors do not automatically have access to files that reside only on the driver.  In earlier tests the backport failed when the descriptor file existed on the driver but not on the executors, leading to a `Malformed Protobuf message detected during parsing`.  To address this, the backport introduces an additional parameter, `binaryDescriptorSet: Option[Array[Byte]]`, on both `ProtobufDataToCatalyst` and `CatalystDataToProtobuf`.  When defined, the bytes are captured on the driver and serialized along with the expression, allowing each executor to build the descriptor in memory without reading a file.  New overloads of `from_protobuf` and `to_protobuf` accept a `byte[]` containing the descriptor set.  These convenience methods are analogous to the compiled‑class and descriptor‑file overloads but avoid the need to distribute files manually (for example by calling `spark.sparkContext.addFile`).  In practice the user can load the `.desc` file on the driver, call `scala.io.Source.fromFile(path).map(_.toByte).toArray` to obtain the bytes and pass them to the function.

Internally, when `binaryDescriptorSet` is defined the expression builds the message descriptor from the byte array rather than from a file.  Otherwise it calls `ProtobufUtils.buildDescriptor` with either the path to the descriptor file or the Java class name as before.  Because the descriptor bytes travel with the serialized plan, all executors see the same schema and can parse or serialise Protobuf messages without file‑related errors.

## Execution path

### Deserialising Protobuf data (`from_protobuf`)

At run time, the `from_protobuf` expression receives a binary input (`Array[Byte]`), parses it into a Protobuf message and converts it into a Catalyst row:

* **Parsing Protobuf bytes.**  In the `nullSafeEval` method, the code uses `DynamicMessage.parseFrom(messageDescriptor, binary)` to construct a `DynamicMessage` instance【222086645450284†L82-L96】.  This method uses the previously loaded descriptor to understand the layout of the binary payload.  It also checks the unknown‑fields map to detect when an incoming payload contains unknown fields whose numeric identifiers clash with known fields, raising a schema‑mismatch error【222086645450284†L82-L96】.

* **Mode‑dependent error handling.**  The parse mode is determined by `protobufOptions.parseMode`.  In permissive mode the expression returns `null` when parsing fails; in fail‑fast mode it throws a `RuntimeException` via `QueryExecutionErrors.malformedProtobufMessageDetectedInMessageParsingError`【222086645450284†L64-L79】.  Unsupported modes result in a compilation error.

* **Converting to Catalyst rows.**  After successfully parsing a `DynamicMessage`, the `ProtobufDataToCatalyst` expression calls `deserializer.deserialize(result)`【222086645450284†L82-L96】.  The `ProtobufDeserializer` constructs a function (`converter`) which, for each message, allocates a `SpecificInternalRow` and writes field values into it.  It uses a `ProtoSchemaHelper` to align Catalyst and Protobuf fields and a set of writer functions to handle primitive values, arrays and maps.  For array fields it creates a `GenericArrayData` or specialised unsafe array and populates it element by element【22263813442226†L70-L101】; for map fields it constructs parallel arrays for keys and values and wraps them in `ArrayBasedMapData`【22263813442226†L103-L142】.  Nested messages are deserialized recursively by obtaining a nested writer using the same helper.  The result of `deserialize` is an `Option[InternalRow]`; the `from_protobuf` expression requires the option to be defined (the filter mechanism is a no‑op in Spark 3.2) and extracts the row.

### Serialising Catalyst values (`to_protobuf`)

The `to_protobuf` expression converts a Catalyst value (normally a struct or nested type) into Protobuf binary:

* **Setting up converters.**  In `CatalystDataToProtobuf` the `protoDescriptor` is obtained in the same way as for deserialisation【629934917462351†L40-L44】.  A `ProtobufSerializer` is then instantiated with the input Catalyst data type, the descriptor and the input’s nullability【629934917462351†L40-L48】.  The serializer builds a root converter: if the input is nullable, it wraps the converter in a null check; otherwise it uses the base converter directly【774038837227391†L50-L69】.

* **Field conversion logic.**  For each field, `ProtobufSerializer` uses `newConverter` to generate a writer that reads a value from a Catalyst row (via `SpecializedGetters`) and produces the appropriate Protobuf representation.  The writer switches on the Catalyst type and the target Protobuf field’s Java type: for example, boolean, integral and floating‑point types are copied directly; strings map to `ENUM` or `STRING` depending on the Protobuf field type; binary data maps to `BYTE_STRING`; timestamps are converted to Protobuf `Timestamp` messages by splitting the microseconds into seconds and nanos; arrays and maps are converted by iterating over elements and creating repeated fields or map‑entry messages【774038837227391†L75-L169】.  For nested structs it recursively builds a nested converter and delegates to it【774038837227391†L133-L137】.  Durations represented as Spark’s day‑time interval type are converted to Protobuf `Duration` messages and normalised【774038837227391†L170-L186】.

* **Building the Protobuf message.**  The root converter created by `newStructConverter` uses a `ProtoSchemaHelper` to align Catalyst and Protobuf fields, validates that required Protobuf fields are present and iterates over the input row’s fields.  For each non‑null field it applies the corresponding field converter and calls `DynamicMessage.Builder.setField` to populate the Protobuf message.  Missing required fields are filled with their default values【774038837227391†L134-L168】.  When the conversion completes, the builder is assembled and returned to the caller, which then calls `toByteArray` on the `DynamicMessage`【629934917462351†L46-L48】 to produce the final binary column.

## Summary

The **spark‑protobuf‑backport** library adapts the Protobuf data source introduced in Spark 3.4 to run on Spark 3.2.1.  It infers a Spark SQL schema from a Protobuf descriptor using `SchemaConverters`, constructs Catalyst expressions (`ProtobufDataToCatalyst` and `CatalystDataToProtobuf`) to perform deserialisation and serialisation, and registers user‑friendly API functions.  Internally it uses `DynamicMessage` to parse and build Protobuf messages and implements converter functions to map between Catalyst types and Protobuf fields.  Optional support for *binary descriptor sets* eliminates executor‑side file dependencies and makes it possible to run jobs on clusters where only the driver can access the `.desc` file.  By providing these building blocks in a standalone JAR, the backport lets users use Protobuf with older Spark versions while following the semantics and API of Spark 3.4.