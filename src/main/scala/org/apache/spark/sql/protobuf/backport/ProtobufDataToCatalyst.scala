/*
 * Backport of Spark 3.4's ProtobufDataToCatalyst to Spark 3.2.1.
 *
 * This version has been enhanced to support deserializing Protobuf
 * binaries directly into Spark's InternalRow without going through
 * DynamicMessage when a compiled Java class is available.  When the
 * message name points at a generated Protobuf Java class (and neither
 * a descriptor file path nor a binary descriptor set is provided) we
 * use reflection to load the class, parse the binary using the
 * class's static `parseFrom` method and then convert the resulting
 * message into an InternalRow using a RowConverter generated by
 * fastproto.ProtoToRowGenerator.  This avoids the overhead of
 * DynamicMessage and allows users to work with compiled messages.
 */

package org.apache.spark.sql.protobuf.backport

import scala.collection.JavaConverters._
import scala.util.control.NonFatal

import com.google.protobuf.DynamicMessage
import com.google.protobuf.{Message => PbMessage}

import org.apache.spark.sql.catalyst.expressions.{ExpectsInputTypes, Expression, UnaryExpression}
import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, CodeGenerator, ExprCode}
import org.apache.spark.sql.catalyst.util.{FailFastMode, ParseMode, PermissiveMode}
import org.apache.spark.sql.types.{AbstractDataType, BinaryType, DataType}
import org.apache.spark.unsafe.types.UTF8String

import org.apache.spark.sql.protobuf.backport.utils.{ProtobufOptions, ProtobufUtils, SchemaConverters}
import org.apache.spark.sql.protobuf.backport.shims.{QueryCompilationErrors, QueryExecutionErrors}
import fastproto.{ProtoToRowGenerator, RowConverter}

/**
 * A Catalyst expression that deserializes a Protobuf binary column into a
 * Catalyst value.  If parsing fails it either returns null (permissive mode)
 * or throws an exception (fail‑fast mode).
 *
 * When a compiled Protobuf class is available (i.e. `messageName` refers
 * directly to a Java class and both `descFilePath` and `binaryDescriptorSet`
 * are empty) the binary is parsed using the class's `parseFrom` method and
 * converted into an InternalRow via a generated [[fastproto.RowConverter]].
 * Otherwise this falls back to Spark's original DynamicMessage‑based
 * deserialization path.
 *
 * @param child        The binary column to deserialize.
 * @param messageName  The fully qualified message name or Java class name.
 * @param descFilePath Optional path to a serialized descriptor file.  If
 *                     provided the descriptor will be loaded from the file;
 *                     otherwise `messageName` is treated as a Java class name.
 * @param options      Reader options; currently supports "mode" (permissive|failfast)
 *                     and "recursive.fields.max.depth".
 * @param binaryDescriptorSet Optional binary descriptor set.  If defined, this
 *                     descriptor will be used to build the message descriptor
 *                     instead of reading from a file on the executors.  This
 *                     allows the descriptor to be serialized with the
 *                     expression and avoids file availability issues.
 */
private[backport] case class ProtobufDataToCatalyst(
    child: Expression,
    messageName: String,
    descFilePath: Option[String] = None,
    options: Map[String, String] = Map.empty,
    binaryDescriptorSet: Option[Array[Byte]] = None)
    extends UnaryExpression
    with ExpectsInputTypes {

  override def inputTypes: Seq[AbstractDataType] = Seq(BinaryType)

  override lazy val dataType: DataType =
    SchemaConverters.toSqlType(messageDescriptor, protobufOptions).dataType

  override def nullable: Boolean = true

  private lazy val protobufOptions = ProtobufOptions(options)

  @transient private lazy val messageDescriptor: com.google.protobuf.Descriptors.Descriptor =
    binaryDescriptorSet match {
      case Some(bytes) => ProtobufUtils.buildDescriptorFromBytes(bytes, messageName)
      case None => ProtobufUtils.buildDescriptor(messageName, descFilePath)
    }

  // Cache the set of known field numbers to detect clashes from unknown fields.
  @transient private lazy val fieldsNumbers =
    messageDescriptor.getFields.asScala.map(f => f.getNumber).toSet

  // Legacy deserializer using DynamicMessage for fallback path.
  @transient private lazy val deserializer = new ProtobufDeserializer(messageDescriptor, dataType)

  // If we are using the DynamicMessage path we store the result here.  This is
  // transient because DynamicMessage is not serializable and the expression is.
  @transient private var result: DynamicMessage = _

  // Determine parse mode once up front.
  @transient private lazy val parseMode: ParseMode = {
    val mode = protobufOptions.parseMode
    if (mode != PermissiveMode && mode != FailFastMode) {
      throw QueryCompilationErrors.parseModeUnsupportedError(prettyName, mode)
    }
    mode
  }

  /**
   * Lazily attempt to load the compiled Protobuf class if the messageName
   * refers to a Java class.  Only do this when no descriptor file or binary
   * descriptor set is provided, otherwise messageName refers to a descriptor
   * symbol rather than a class.  Any errors during class loading are
   * swallowed and result in [[None]], causing the DynamicMessage path to be
   * used instead.
   */
  @transient private lazy val messageClassOpt: Option[Class[_ <: PbMessage]] = {
    (descFilePath, binaryDescriptorSet) match {
      case (None, None) =>
        try {
          Some(Class.forName(messageName).asInstanceOf[Class[_ <: PbMessage]])
        } catch {
          case _: Throwable => None
        }
      case _ => None
    }
  }

  /**
   * Lazily create a [[fastproto.RowConverter]] for the compiled message class.
   * The converter uses the descriptor corresponding to this expression (from
   * either the provided binary descriptor set or `ProtobufUtils.buildDescriptor`)
   * so that the schema matches the Catalyst data type.  If the message class
   * cannot be loaded this will be [[None]].
   */
  @transient private lazy val rowConverterOpt: Option[RowConverter[PbMessage]] =
    messageClassOpt.map { cls =>
      // Use the descriptor used to compute the Catalyst schema so that
      // the row converter's schema matches the Catalyst data type.  Cast
      // the generated converter to RowConverter[PbMessage] since PbMessage
      // is a supertype of all generated protobuf classes.
      val desc: com.google.protobuf.Descriptors.Descriptor = messageDescriptor
      ProtoToRowGenerator
        .generateConverter(desc, cls.asInstanceOf[Class[PbMessage]])
        .asInstanceOf[RowConverter[PbMessage]]
    }

  /**
   * Parse a Protobuf binary into a compiled message if the message class is
   * available.  Uses reflection to call the static parseFrom(byte[]) method.
   * Returns [[Some]] when parsing succeeds, otherwise [[None]].
   */
  private def parseCompiled(binary: Array[Byte]): Option[PbMessage] = {
    messageClassOpt.flatMap { cls =>
      try {
        val method = cls.getMethod("parseFrom", classOf[Array[Byte]])
        val msg = method.invoke(null, binary).asInstanceOf[PbMessage]
        Some(msg)
      } catch {
        case _: Throwable => None
      }
    }
  }

  /**
   * Handle an exception according to the configured parse mode.
   */
  private def handleException(e: Throwable): Any = {
    parseMode match {
      case PermissiveMode => null
      case FailFastMode =>
        throw QueryExecutionErrors.malformedProtobufMessageDetectedInMessageParsingError(e)
      case _ =>
        throw QueryCompilationErrors.parseModeUnsupportedError(prettyName, parseMode)
    }
  }

  override def nullSafeEval(input: Any): Any = {
    val binary = input.asInstanceOf[Array[Byte]]
    try {
      // If a rowConverter is defined, attempt to parse using the compiled class
      rowConverterOpt match {
        case Some(converter) =>
          parseCompiled(binary) match {
            case Some(msg) =>
              // Directly convert the compiled message into an InternalRow
              return converter.convert(msg)
            case None => // fallback to DynamicMessage path
          }
        case None => // fallback to DynamicMessage path
      }
      // DynamicMessage path: parse the binary using the message descriptor
      result = DynamicMessage.parseFrom(messageDescriptor, binary)
      // Check for unknown fields that clash with known field numbers; this indicates
      // a mismatch between writer and reader schemas.  Use findFieldByNumber
      // instead of indexing into getFields by number, because Protobuf field
      // numbers are 1‑based and may not align with the list index.
      result.getUnknownFields.asMap().keySet().asScala.find(fieldsNumbers.contains(_)) match {
        case Some(number) =>
          val conflictingField = Option(messageDescriptor.findFieldByNumber(number))
            .getOrElse(messageDescriptor.getFields.get(number - 1))
          throw QueryCompilationErrors.protobufFieldTypeMismatchError(conflictingField.toString)
        case None => // no clash
      }
      val deserialized = deserializer.deserialize(result)
      require(
        deserialized.isDefined,
        "Protobuf deserializer cannot return an empty result because filters are not pushed down")
      // Convert any java.lang.String values in the deserialized result into UTF8String.
      // This avoids ClassCastException when Spark expects UTF8String for StringType fields.
      val rawValue = deserialized.get
      def toInternalString(value: Any, dt: DataType): Any = (value, dt) match {
        case (null, _) => null
        case (s: String, _: org.apache.spark.sql.types.StringType) => UTF8String.fromString(s)
        case (arr: scala.collection.Seq[_], at: org.apache.spark.sql.types.ArrayType) =>
          // Convert each element according to the element type
          arr.map(elem => toInternalString(elem, at.elementType))
        case (row: org.apache.spark.sql.catalyst.InternalRow, st: org.apache.spark.sql.types.StructType) =>
          val newValues = st.fields.zipWithIndex.map { case (field, idx) =>
            toInternalString(row.get(idx, field.dataType), field.dataType)
          }
          org.apache.spark.sql.catalyst.InternalRow.fromSeq(newValues)
        case (m: Map[_, _], mt: org.apache.spark.sql.types.MapType) =>
          m.map { case (k, v) =>
            toInternalString(k, mt.keyType) -> toInternalString(v, mt.valueType)
          }
        case _ => value
      }
      toInternalString(rawValue, dataType)
    } catch {
      case NonFatal(e) => handleException(e)
    }
  }

  override def prettyName: String = "from_protobuf"

  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    val expr = ctx.addReferenceObj("this", this)
    nullSafeCodeGen(
      ctx,
      ev,
      eval => {
        val result = ctx.freshName("result")
        val dt = CodeGenerator.boxedType(dataType)
        s"""
           |$dt $result = ($dt) $expr.nullSafeEval($eval);
           |if ($result == null) {
           |  ${ev.isNull} = true;
           |} else {
           |  ${ev.value} = $result;
           |}
           |""".stripMargin
      }
    )
  }

  override protected def withNewChildInternal(newChild: Expression): ProtobufDataToCatalyst =
    copy(child = newChild)
}