package org.apache.spark.sql.protobuf.backport

import com.google.protobuf.Message
import fastproto.ProtoToRowGenerator
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.protobuf.backport.utils.{ProtobufUtils, SchemaConverters}
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{DataFrame, Dataset}

import scala.reflect.ClassTag

/**
 * Helper object for converting a [[Dataset]] of compiled Protobuf messages
 * into a [[DataFrame]] backed by Spark's internal row format.  This avoids
 * using DynamicMessage and instead utilises the compiled Java class along
 * with a simple [[fastproto.Parser]] generated on the fly.  The
 * resulting DataFrame will have a schema derived from the Protobuf
 * descriptor and is suitable for further Spark SQL processing.
 */
object ProtoDataset {
  /**
   * Convert a [[Dataset]] of Protobuf messages into a [[DataFrame]].  The
   * provided dataset must contain instances of a compiled Protobuf class
   * generated by protoc.  The method infers the descriptor via
   * [[ProtobufUtils.buildDescriptorFromJavaClass]] and generates a
   * [[fastproto.Parser]] to transform each message into an
   * [[org.apache.spark.sql.catalyst.InternalRow]].  Under the hood this
   * delegates to [[SparkSession.internalCreateDataFrame]] to construct the
   * DataFrame.
   *
   * @param ds the input dataset of Protobuf messages
   * @tparam T the concrete message type
   * @return a DataFrame representing the messages as rows
   */
  def toDataFrame[T <: Message : ClassTag](ds: Dataset[T]): DataFrame = {
    val spark = ds.sparkSession
    val clazz = implicitly[ClassTag[T]].runtimeClass.asInstanceOf[Class[T]]
    // Build a descriptor from the message class name.  This handles class
    // loading and shaded Protobuf classes via ProtobufUtils.
    val descriptor = ProtobufUtils.buildDescriptorFromJavaClass(clazz.getName)
    // Generate schema using SchemaConverters
    val schema = SchemaConverters.toSqlType(descriptor).dataType.asInstanceOf[StructType]
    // Generate a parser using our implementation (caching is handled internally)
    val parser = ProtoToRowGenerator.generateParser(descriptor, clazz, schema)
    val rows: RDD[org.apache.spark.sql.catalyst.InternalRow] = ds.rdd.map { msg =>
      // Cast to MessageParser to access message conversion methods
      parser.parse(msg)
    }
    // Use internalCreateDataFrame to avoid schema inference.  This helper is
    // defined in the same package as SparkSession so that it can call the
    // package-private method.
    spark.internalCreateDataFrame(rows, schema)
  }
}
