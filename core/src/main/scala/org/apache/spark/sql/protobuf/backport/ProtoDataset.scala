package org.apache.spark.sql.protobuf.backport

import com.google.protobuf.Message
import fastproto.ProtoToRowGenerator
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.protobuf.backport.utils.ProtobufUtils
import org.apache.spark.sql.{DataFrame, Dataset}

import scala.reflect.ClassTag

/**
 * Helper object for converting a [[Dataset]] of compiled Protobuf messages
 * into a [[DataFrame]] backed by Spark's internal row format.  This avoids
 * using DynamicMessage and instead utilises the compiled Java class along
 * with a simple [[fastproto.RowConverter]] generated on the fly.  The
 * resulting DataFrame will have a schema derived from the Protobuf
 * descriptor and is suitable for further Spark SQL processing.
 */
object ProtoDataset {
  /**
   * Convert a [[Dataset]] of Protobuf messages into a [[DataFrame]].  The
   * provided dataset must contain instances of a compiled Protobuf class
   * generated by protoc.  The method infers the descriptor via
   * [[ProtobufUtils.buildDescriptorFromJavaClass]] and generates a
   * [[fastproto.RowConverter]] to transform each message into an
   * [[org.apache.spark.sql.catalyst.InternalRow]].  Under the hood this
   * delegates to [[DevInterface.internalCreateDataFrame]] to construct the
   * DataFrame.
   *
   * @param ds the input dataset of Protobuf messages
   * @tparam T the concrete message type
   * @return a DataFrame representing the messages as rows
   */
  def toDataFrame[T <: Message : ClassTag](ds: Dataset[T]): DataFrame = {
    val spark = ds.sparkSession
    val clazz = implicitly[ClassTag[T]].runtimeClass.asInstanceOf[Class[T]]
    // Build a descriptor from the message class name.  This handles class
    // loading and shaded Protobuf classes via ProtobufUtils.
    val descriptor = ProtobufUtils.buildDescriptorFromJavaClass(clazz.getName)
    // Generate a converter using our implementation (caching is handled internally)
    val converter = ProtoToRowGenerator.generateConverter(descriptor, clazz)
    val schema = converter.schema
    val rows: RDD[org.apache.spark.sql.catalyst.InternalRow] = ds.rdd.map { msg =>
      converter.convert(msg)
    }
    // Use internalCreateDataFrame to avoid schema inference.  This helper is
    // defined in the same package as SparkSession so that it can call the
    // package-private method.
    spark.internalCreateDataFrame(rows, schema)
  }
}
